# LLM推理框架(vLLM/SGLang)入门Notebook练习（2026年第1期）

**Author:** kaiyuan
**Date:** 2026-01-28
**Original Link:** https://zhuanlan.zhihu.com/p/1999518738303693534

---

## 目录

为了帮助初学者快速掌握 LLM 推理框架（vLLM/SGLang）的基本概念，此前我已将相关知识与概念按逻辑梳理至InfraTech代码库中；本期则重点整理并新增了若干配套的notebook练习，助力读者更快速地理解相关概念，具体涵盖的内容如下：

1. [KV Cache计算](#1-kv-cache计算)
2. [LLM推理采样(Sampling)](#2-llm推理采样sampling)
3. [ChunkedPrefill&FlashDecoding原理详解](#3-chunkedprefillflashdecoding原理详解)
4. [手搓基础调度器（Scheduler）](#4-手搓基础调度器scheduler)
5. [理解Radix Attention原理](#5-理解radix-attention原理)
6. [Ulysses基本原理](#6-ulysses基本原理)
7. [投机推理（Speculative Decoding）](#7-投机推理speculative-decoding)
8. [LoRA&Multi-LoRA](#8-loramulti-lora)
9. [推理非确定性运算理解](#9-推理非确定性运算理解)
10. [ZMQ库操作入门](#10-zmq库操作入门)

---

## InfraTech代码仓库

[CalvinXKY/InfraTech: 分享AI Infra知识&代码练习：PyTorch/vLLM/SGLang框架入门⚡️、性能加速🚀、大模型基础🧠、AI软硬件🔧等](https://github.com/CalvinXKY/InfraTech)

---

## 1. KV Cache计算

### 介绍
前置缓存（prefix caching）可有效降低prefill阶段的计算开销，显著提升首token生成性能，目前已被行业广泛采用。其原理是通过跨请求/对话的kv cache复用来减少计算。但使用KV cache对计算量的影响有多大？本文将通过具体用例对此问题展开分析与解读。

### 相关文章
- vLLM的prefix cache为何零开销
- 高效推理的核心：vLLM V1 KV cache 管理机制剖析

### 案例特点
针对Attention（MLA）模块和FFN（MoE）模块的flops计算，通过增加已计算的前缀缓存（Prefix Cache）长度，观察计算量的变化情况。

### 代码位置
`InfraTech/llm_infer/attention_mla_flops_with_prefix_cache.ipynb`

---

## 2. LLM推理采样(Sampling)

### 介绍
LLM推理采样（Sampling）常见操作包括：Temperature/Topk/TopP/Penalty等，本练习主要：
- 理解推理采样概念
- 调节温度对采样的影响
- 采样顺序对结果的影响

### 相关文章
LLM推理采样(Sampling)常见知识概览

### 案例特点
以简易案例讲解常见采样操作，并对关键操作做可视化处理，更易理解。

### 代码位置
`InfraTech/llm_infer/LLM_sampling.ipynb`

---

## 3. ChunkedPrefill&FlashDecoding原理详解

### 介绍
在LLM**长序列**推理中，常会用到分块预填充（**Chunked Prefill**）和快速解码（**Flash Decoding**）来提升性能。虽然两个特性都是将长序列进行拆分计算，但两者的应用场景和计算方式差异较大。结合文章理解ChunkedPrefill和FlashDecoding的基本原理，通过示例代码理解两种**分块运算**的计算过程。

### 相关文章
推理长序列利器：ChunkedPrefill&FlashDecoding原理详解

### 案例特点
无GPU运行，通过单机模拟多机运行，计算过程有详细打印。

### 代码位置
`InfraTech/llm_infer/chunked_prefill_and_flash_decoding.ipynb`

---

## 4. 手搓基础调度器（Scheduler）

### 介绍
带读者构建一个最基础的调度器，通过抓住概念核心，达到事半功倍的效果。

### 相关文章
vLLM Scheduler逻辑难啃？先手搓一个基础调度器

### 案例特点
将调度器的执行步骤进行可视化呈现，助力理解请求的完整处理流程。

### 代码位置
`InfraTech/llm_infer/vllm_basic_scheduler.ipynb`

---

## 5. 理解Radix Attention原理

### 介绍
基于RadixAttention的KV cache管理模块是SGLang的核心组件之一。本案例讲解RadixAttention的核心机制。

### 相关文章
手撕SGLang KV Cache核心逻辑：快速理解RadixAttention

### 案例特点
操作过程中可将RadixTree的节点信息打印出来，便于理解。

### 代码位置
`InfraTech/llm_infer/sglang_radix_attention.ipynb`

---

## 6. Ulysses基本原理

### 介绍
Ulysses的全称是DeepSpeed‑Ulysses，其核心逻辑：开启序列并行后，在多头Attention运算之前，多个 GPU设备之间会进行数据交换，使单个GPU能够拥有完整的序列；本用例示意Ulysses计算过程。

### 相关文章
推理Ulysses并行优化与DeepSeekV3/V3.2实践

### 案例特点
无GPU运行，通过单机模拟多机计算。

### 代码位置
`InfraTech/llm_infer/ulysses_mha_demo.ipynb`

---

## 7. 投机推理（Speculative Decoding）

### 介绍
投机推理（Speculative Decoding）是一种高效的推理方式，可提升推理性能（如ITL）、降低推理成本，在特定场景下能够提升用户体感。示例内容：
- 投机推理原理演示
- Ngram原理演示
- vLLM的示例

### 相关文章
LLM提速利器：投机推理的原理与常见方案

### 案例特点
通过模拟 LLM 运行构造用例，辅助读者理解投机推理的核心过程。

### 代码位置
`InfraTech/llm_infer/speculative_decoding.ipynb`

---

## 8. LoRA&Multi-LoRA

### 介绍
Multi-LoRA(Low-Rank Adaptation)是大模型部署中的常见方案，可有效解决多场景下训练 / 推理成本过高的问题。本案例将以一个分类任务为例，对该方案详细解析。

### 相关文章
从LoRA到Multi-LoRA：原理&代码实践

### 案例特点
以手写体识别（数字 0~9）为任务场景开展端到端训练，通过LoRA微调可观测到识别能力的提升，且全程基于CPU完成训练流程。

### 代码位置
`InfraTech/multi_lora`

---

## 9. 推理非确定性运算理解

### 介绍
在Debug、强化学习、精度对齐等场景下，我们有时会期望模型具备重复输出的能力（即可复现性）。但非确定性运算（Non-deterministic Computation）会导致推理过程中结果不可控。本文将通过Triton库构建一个规约（reduction）运算，帮助理解这一现象背后的原理。

### 相关文章
推理的非确定性运算及vLLM/SGLang控制方式

### 案例特点
涵盖FP16、FP32、PyTorch默认运算的对比分析。

### 代码位置
因为本案例需要使用GPU运算，所以用例归类到了BasicCUDA库，文件位置：`./triton/nondeterministic_reduction.ipynb`

---

## 10. ZMQ库操作入门

### 介绍
ZMQ是ZeroMQ的缩写，全称为Zero Message Queue，是一款高性能、可扩展的消息传递库，主要用于构建分布式及并发应用程序。ZMQ是推理框架中常见的通信库，常用于关键模块之间的通信，具体应用示例如下：
- API server与LLM之间通信
- LLM模块与Engine Core的通信
- DP并行场景下，不同DP节点间的数据通信

### 内容
理解常见几种通信模式。

### 案例特点
除直观演示核心原理外，还在示例中绘制了多款通信模式的拓扑示意图。

### 代码位置
`InfraTech/llm_infer/zmq_practice.ipynb`

---

## 总结

本文整理了10个LLM推理框架入门的Notebook练习，涵盖了从基础的KV Cache计算、采样机制，到高级的并行优化、投机推理等多个核心主题。每个练习都配有详细的代码实现和相关文章解析，适合初学者快速掌握vLLM/SGLang框架的核心概念。

---

## 参考资料

1. [InfraTech代码库](https://github.com/CalvinXKY/InfraTech)
2. [BasicCUDA代码库](https://github.com/CalvinXKY/BasicCUDA)

---

**作者：** @kaiyuan
**内容持续更新，欢迎关注**
