# moe_sum 算子 - 从入门到精通

> 目标读者：LLM 初学者 | 预计阅读时间：15 分钟

---

## 第一部分：什么是 moe_sum？（5分钟）

### 1.1 用生活例子理解

想象你是一家公司的老板，遇到了一个复杂的问题需要解决。你有两种选择：

**传统模型（全激活）**：
- 让所有 100 个员工都来思考这个问题
- 浪费时间，效率低下

**MoE 模型（混合专家）**：
- 先让"门控"看看问题的性质
- 只叫最相关的 4 个专家来处理
- 最后把这 4 个专家的建议**汇总**成一个完整答案

```
┌─────────────────────────────────────────────────────────┐
│                      问题：翻译这句话                      │
└─────────────────────────────────────────────────────────┘
                          ↓
        ┌─────────────────┴─────────────────┐
        │          门控网络 (Router)          │
        │    分析问题类型：英语→中文翻译      │
        └─────────────────┬─────────────────┘
                          ↓
    ┌─────────────────────┼─────────────────────┐
    │                     │                     │
   [✓]                   [✗]                   [✗]
  翻译专家               数学专家               编程专家
    ↓                     │                     │
 输出建议                  │                     │
    │                     │                     │
    └─────────────────────┴─────────────────────┘
                          ↓
                ┌─────────────────┐
                │   moe_sum 聚合   │  ← 把所有选中专家的建议汇总
                └─────────────────┘
                          ↓
              "最终的翻译结果"
```

### 1.2 技术定义

**`moe_sum`** 是一个计算操作，它的作用是：

> 把多个专家的输出**按位置求和**，合并成一个最终输出。

#### 关键概念对照表

| 生活概念 | 技术术语 | 说明 |
|---------|---------|------|
| 问题 | Token | 模型处理的基本单位 |
| 员工数量 | 总专家数 | 模型中所有专家的总数 |
| 被叫到的员工 | 使用的专家数 (n_expert_used) | 实际参与计算的专家 |
| 汇总建议 | moe_sum | 求和聚合操作 |
| 最终答案 | 输出张量 | [hidden_dim, tokens] |

---

## 第二部分：数据怎么流动？（5分钟）

### 2.1 输入是什么？

`moe_sum` 的输入是一个**3维张量**：

```
形状: [hidden_dim, n_experts_used, tokens]
```

让我们用具体数字来理解：

```
假设：
- hidden_dim = 4096    (每个 token 用 4096 个数表示)
- n_experts_used = 4   (选了 4 个专家)
- tokens = 512         (处理 512 个 token)

输入形状: [4096, 4, 512]
```

#### 可视化输入结构

```
输入张量 [4096, 4, 512]

Token 0 的数据:
┌─────────────────────────────────────┐
│ Expert 0: [4096 个数值]              │
│ Expert 1: [4096 个数值]              │  ← 4个专家各自对token 0的输出
│ Expert 2: [4096 个数值]              │
│ Expert 3: [4096 个数值]              │
└─────────────────────────────────────┘

Token 1 的数据:
┌─────────────────────────────────────┐
│ Expert 0: [4096 个数值]              │
│ Expert 1: [4096 个数值]              │
│ Expert 2: [4096 个数值]              │
│ Expert 3: [4096 个数值]              │
└─────────────────────────────────────┘

... (重复 512 次)
```

### 2.2 输出是什么？

输出是一个**2维张量**：

```
形状: [hidden_dim, tokens]
即: [4096, 512]
```

每个 token 只有一行数据，是所有专家输出的**求和结果**。

### 2.3 计算过程图解

```
输入: [4096, 4, 512]
        ↓
┌───────────────────────────────────────────┐
│  对 Token 0 的第 i 个维度求和:             │
│                                           │
│  output[0][i] =                            │
│    input[0][i][0] +  Expert 0 的贡献     │
│    input[1][i][0] +  Expert 1 的贡献     │
│    input[2][i][0] +  Expert 2 的贡献     │
│    input[3][i][0]    Expert 3 的贡献     │
└───────────────────────────────────────────┘
        ↓
输出: [4096, 512]

每个位置都是对应位置专家输出的简单求和
```

#### 简化示例（数值演示）

假设只有 2 个维度，2 个专家，1 个 token：

```
输入: [2, 2, 1]

Expert 0 的输出: [1.5, 2.3]
Expert 1 的输出: [0.8, 1.9]

           维度 0   维度 1
           ─────── ───────
Expert 0:    1.5     2.3
Expert 1:    0.8     1.9
           ─────── ───────
求和结果:    2.3     4.2   ← moe_sum 的输出

输出: [2.3, 4.2]
```

---

## 第三部分：在 MoE 中的位置（3分钟）

### 3.1 完整的 MoE 计算流程

```
┌─────────────────────────────────────────────────────────────────┐
│                        MoE 层的完整流程                          │
└─────────────────────────────────────────────────────────────────┘

输入: [4096, 512]
  ↓
┌──────────────────┐
│  1. 门控网络      │  → 计算每个专家的重要性分数
│  (Gating)        │  → 输出: [64, 512] (假设64个专家)
└──────────────────┘
  ↓
┌──────────────────┐
│  2. TopK 选择    │  → 选出最重要的 4 个专家
│  (Argsort)       │  → 输出: 专家索引 + 权重
└──────────────────┘
  ↓
┌──────────────────┐
│  3. 专家 FFN     │  → 每个选中的专家独立计算
│  (Expert FFN)    │  → 输出: [4096, 4, 512]
└──────────────────┘
  ↓
┌──────────────────┐
│  4. moe_sum ◁───│  ← 聚合所有专家输出
│  (聚合)          │  → 输出: [4096, 512]
└──────────────────┘
  ↓
输出: [4096, 512]
```

### 3.2 为什么需要 moe_sum？

如果没有 `moe_sum`，我们需要这样做：

```cpp
// 传统方式：循环相加
ggml_tensor * result = expert_outputs[0];
result = ggml_add(result, expert_outputs[1]);  // 需要多次计算图操作
result = ggml_add(result, expert_outputs[2]);
result = ggml_add(result, expert_outputs[3]);
```

使用 `moe_sum`：

```cpp
// 优化方式：一次操作
ggml_tensor * result = ggml_moe_sum(ctx, expert_outputs, n_expert_used);
```

**优势**：
- ✅ 单一算子，减少计算图开销
- ✅ GPU 内核可以融合内存访问
- ✅ 针对不同场景有特化优化

---

## 第四部分：代码实现解析（高级内容）

### 4.1 算子定义

```cpp
// ggml/include/ggml.h:570
enum ggml_op {
    ...
    GGML_OP_MOE_SUM,  // MoE 专家输出聚合
    ...
};
```

### 4.2 CPU 实现（简化理解）

```cpp
void ggml_compute_forward_moe_sum(
        const ggml_compute_params * params,
        ggml_tensor * dst) {

    // 输入: [hidden_dim, n_experts_used, tokens]
    // 输出: [hidden_dim, tokens]

    // 第1步：把输出清零
    memset(dst->data, 0, ggml_nbytes(dst));

    // 第2步：创建输出视图（不复制数据）
    ggml_tensor dst_view = create_view(dst);

    // 第3步：依次把每个专家的输出加到结果上
    for (int i = 0; i < n_expert_used; i++) {
        // 创建当前专家的视图
        ggml_tensor expert_view = create_expert_view(src0, i);
        dst_view.src[0] = &expert_view;

        // 调用加法操作累加
        ggml_compute_forward_add(params, &dst_view);
    }
}
```

**关键技巧：张量视图 (Tensor View)**

```
原始数据: [4096, 4, 512]
              ↓
         创建视图（零成本）

视图0: 指向 Expert 0 的数据 [4096, 512]
视图1: 指向 Expert 1 的数据 [4096, 512]
视图2: 指向 Expert 2 的数据 [4096, 512]
视图3: 指向 Expert 3 的数据 [4096, 512]

无需复制数据，只是改变"看待"数据的方式
```

### 4.3 GPU 实现（三种内核）

GPU 实现根据数据规模和特征选择不同策略：

#### 决策树

```
输入数据
    ↓
是 FP16 吗？
    ├── 是 ──→ token > 256 且 dim 是 8 的倍数？
    │           ├── 是 ──→ [向量化内核] ★最快★
    │           └── 否 ──→ 继续 ↓
    │
    └── 否 ──→ 继续 ↓
              ↓
         token > 128？
         ├── 是 ──→ [Warp-Per-Token 内核]
         └── 否 ──→ [标准内核]
```

#### 内核对比

| 内核 | 适用场景 | 优化技巧 |
|------|----------|----------|
| **向量化内核** | FP16 + 大批量 | 16元素打包、128位加载 |
| **Warp-Per-Token** | 中等规模 | 每32线程处理1个token |
| **标准内核** | 小批量 | 简单2D网格 |

#### 向量化内核原理

```
标准加载（一次读1个）:
[1.2] [3.4] [5.6] [7.8] ...
  ↓     ↓     ↓     ↓
读4次

向量化加载（一次读16个）:
[1.2] [3.4] [5.6] [7.8] [9.0] ... [15.6]
  └─────────────────────┘
        读1次 (128-bit)

速度提升约 4 倍
```

---

## 第五部分：快速参考

### 输入输出速查

```
输入: [hidden_dim, n_experts_used, tokens]
输出: [hidden_dim, tokens]

操作: 对每个 token，在 hidden_dim 维度上对所有专家求和
```

### 代码速查

```cpp
// 创建 moe_sum 算子
ggml_tensor * result = ggml_moe_sum(
    ctx,           // 计算上下文
    experts,       // 输入: [hidden_dim, n_experts_used, tokens]
    n_expert_used  // 专家数量
);
```

### 相关文件

| 文件 | 内容 |
|------|------|
| `ggml/include/ggml.h:570` | 算子定义 |
| `ggml/src/ggml-cpu/ops.cpp:11013` | CPU 实现 |
| `ggml/src/ggml-dlcu/dl-moesum.cu` | GPU 实现 |

### 延伸阅读

- [MoE 架构详解](../../../docs/for-me/moe-architecture.md) - 完整的 MoE 架构说明
- [Top-K 专家选择](../../../docs/for-me/Top-K-Expert-Selection.md) - 如何选择专家
- [Prefix Cache](../prefix-cache/) - KV Cache 优化技术

---

## 总结

`moe_sum` 是 MoE 模型中**简单但关键**的聚合操作：

1. **作用**：把多个专家的输出按位置求和
2. **输入**：`[hidden_dim, n_experts_used, tokens]`
3. **输出**：`[hidden_dim, tokens]`
4. **优化**：GPU 实现有三种特化内核，根据数据规模自动选择

理解 `moe_sum` 是理解 MoE 推理流程的关键一步！
